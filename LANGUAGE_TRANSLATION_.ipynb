{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Gin7FCLoxRCQmqepYzRKUNNrn6XG3nCH",
      "authorship_tag": "ABX9TyNIjSRM7UxgfURwPCa1hKIR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anaumsharif/LANGUAGE-TRANSLATION/blob/main/LANGUAGE_TRANSLATION_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np  # linear algebra\n",
        "import pandas as pd   # data processing\n",
        "\n",
        "import os\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "yCBpi8Wwlcv1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "z70Y5Fc7kFuN",
        "outputId": "4cadd3ab-a4e3-4ad4-e52c-08891e91f5ce"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8bdaf15d-f6c5-42b5-836d-ad5ec3562266\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8bdaf15d-f6c5-42b5-836d-ad5ec3562266\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Import dataset in google colab\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines =pd.read_csv(\"/content/Hindi_English_Truncated_Corpus .csv\",encoding ='utf-8')"
      ],
      "metadata": {
        "id": "wdPuBhQ7ll7j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JInAOtGMll-B",
        "outputId": "4232e07b-161c-4631-bd2d-24d45fbe7716"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      source                                   english_sentence  \\\n",
              "0        ted  politicians do not have permission to do what ...   \n",
              "1        ted         I'd like to tell you about one such child,   \n",
              "2  indic2012  This percentage is even greater than the perce...   \n",
              "3        ted  what we really mean is that they're bad at not...   \n",
              "4  indic2012  .The ending portion of these Vedas is called U...   \n",
              "\n",
              "                                      hindi_sentence  \n",
              "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
              "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
              "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
              "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
              "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e349f5ef-b1c2-4726-8288-9b3ace8f7efa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>english_sentence</th>\n",
              "      <th>hindi_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ted</td>\n",
              "      <td>politicians do not have permission to do what ...</td>\n",
              "      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ted</td>\n",
              "      <td>I'd like to tell you about one such child,</td>\n",
              "      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>indic2012</td>\n",
              "      <td>This percentage is even greater than the perce...</td>\n",
              "      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ted</td>\n",
              "      <td>what we really mean is that they're bad at not...</td>\n",
              "      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>indic2012</td>\n",
              "      <td>.The ending portion of these Vedas is called U...</td>\n",
              "      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e349f5ef-b1c2-4726-8288-9b3ace8f7efa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e349f5ef-b1c2-4726-8288-9b3ace8f7efa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e349f5ef-b1c2-4726-8288-9b3ace8f7efa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ec733f42-1033-4589-b226-0a85a250b7f6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ec733f42-1033-4589-b226-0a85a250b7f6')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ec733f42-1033-4589-b226-0a85a250b7f6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = lines[lines['source']=='ted']"
      ],
      "metadata": {
        "id": "Zn2klzXTlmAZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines =lines[~pd.isnull(lines['english_sentence'])]\n",
        "lines.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "39N96mRQlmDE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Picking any 25000 rows from the dataset\n",
        "lines = lines.sample(n=25000,random_state=42)\n",
        "lines.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIYSgXz8lmFy",
        "outputId": "227663fc-102c-491e-ef08-be940dbf083e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lowering all the data\n",
        "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: x.lower())\n",
        "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "ITtPWCmXlmIH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing all the  quotes from the data\n",
        "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: re.sub(\"'\",'',x))\n",
        "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: re.sub(\"'\",'',x))"
      ],
      "metadata": {
        "id": "pWz5spZAlmK1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing all the special characters in the data\n",
        "exclude = set(string.punctuation) # set of all special characters\n",
        "# removing\n",
        "lines['english_sentence'] = lines['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
      ],
      "metadata": {
        "id": "f2dWKBR2lmOP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing all the numbers and extra spaces from the data\n",
        "\n",
        "# removing digits\n",
        "remove_digits = str.maketrans('', '', digits)\n",
        "lines['english_sentence']=lines['english_sentence'].apply(lambda x: x.translate(remove_digits))\n",
        "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n",
        "\n",
        "\n",
        "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
        "\n",
        "\n",
        "# removing spaces\n",
        "lines['english_sentence']=lines['english_sentence'].apply(lambda x: x.strip())\n",
        "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.strip())\n",
        "lines['english_sentence']=lines['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "\n",
        "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x : 'START_ '+ x + ' _END')"
      ],
      "metadata": {
        "id": "9-raskN9oxQ_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Get English and Hindi Vocabulary\n",
        "all_eng_words=set()\n",
        "for eng in lines['english_sentence']:\n",
        "    for word in eng.split():\n",
        "        if word not in all_eng_words:\n",
        "            all_eng_words.add(word)\n",
        "\n",
        "all_hindi_words=set()\n",
        "for hin in lines['hindi_sentence']:\n",
        "    for word in hin.split():\n",
        "        if word not in all_hindi_words:\n",
        "            all_hindi_words.add(word)\n",
        "lines['length_eng_sentence']=lines['english_sentence'].apply(lambda x:len(x.split(\" \")))\n",
        "lines['length_hin_sentence']=lines['hindi_sentence'].apply(lambda x:len(x.split(\" \")))"
      ],
      "metadata": {
        "id": "SQ_N62XHoxTc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines=lines[lines['length_eng_sentence']<=20]\n",
        "lines=lines[lines['length_hin_sentence']<=20]\n",
        "max_length_src=max(lines['length_hin_sentence'])\n",
        "max_length_tar=max(lines['length_eng_sentence'])\n",
        "\n",
        "input_words = sorted(list(all_eng_words))\n",
        "target_words = sorted(list(all_hindi_words))\n",
        "num_encoder_tokens = len(all_eng_words)\n",
        "num_decoder_tokens = len(all_hindi_words)\n",
        "num_encoder_tokens, num_decoder_tokens\n",
        "\n",
        "num_decoder_tokens += 1 #for zero padding\n",
        "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
        "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n",
        "lines = shuffle(lines)"
      ],
      "metadata": {
        "id": "JA2JwXBdoxV_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SPLITING DATASET\n"
      ],
      "metadata": {
        "id": "9ux-sadjpGQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "_RzO5Nd8oxbF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = lines['english_sentence'], lines['hindi_sentence']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=42)\n",
        "\n",
        "X_train.to_pickle('X_train.pkl')\n",
        "X_test.to_pickle('X_test.pkl')"
      ],
      "metadata": {
        "id": "uc6Stx4SoxdI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING MODEL"
      ],
      "metadata": {
        "id": "Z5sBSDOFpQCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input ,LSTM,Embedding ,Dense\n",
        "from keras.models import Model"
      ],
      "metadata": {
        "id": "zrrV5tUzoxfm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch(X = X_train, y = y_train, batch_size = 128):\n",
        "    ''' Generate a batch of data '''\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            encoder_input_data = np.zeros((batch_size, max_length_src),dtype='float32')\n",
        "            decoder_input_data = np.zeros((batch_size, max_length_tar),dtype='float32')\n",
        "            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens),dtype='float32')\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n",
        "                for t, word in enumerate(input_text.split()):\n",
        "                    encoder_input_data[i, t] = input_token_index[word] # encoder input seq\n",
        "                for t, word in enumerate(target_text.split()):\n",
        "                    if t<len(target_text.split())-1:\n",
        "                        decoder_input_data[i, t] = target_token_index[word] # decoder input seq\n",
        "                    if t>0:\n",
        "                        # decoder target sequence (one hot encoded)\n",
        "                        # does not include the START_ token\n",
        "                        # Offset by one timestep\n",
        "                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.\n",
        "            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n",
        "\n",
        "latent_dim=300\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "model.summary()\n",
        "\n",
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PDoMcRboxiQ",
        "outputId": "0ef465c7-6d3a-4f2f-9899-c4e9d2ed8135"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, None, 300)            4209000   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, None, 300)            5262300   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 300),                721200    ['embedding[0][0]']           \n",
            "                              (None, 300),                                                        \n",
            "                              (None, 300)]                                                        \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, None, 300),          721200    ['embedding_1[0][0]',         \n",
            "                              (None, 300),                           'lstm[0][1]',                \n",
            "                              (None, 300)]                           'lstm[0][2]']                \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, None, 17541)          5279841   ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 16193541 (61.77 MB)\n",
            "Trainable params: 16193541 (61.77 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n",
        "                    steps_per_epoch = train_samples//batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n",
        "                    validation_steps = val_samples//batch_size)\n",
        "\n",
        "model.save_weights('nmt_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLyhnOZcoxk1",
        "outputId": "21591fea-3fc5-46bf-e5b3-03489e55abc5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-393be5fbb22c>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "154/154 [==============================] - 83s 438ms/step - loss: 6.9490 - val_loss: 6.3633\n",
            "Epoch 2/100\n",
            "154/154 [==============================] - 51s 327ms/step - loss: 6.3067 - val_loss: 6.3128\n",
            "Epoch 3/100\n",
            "154/154 [==============================] - 50s 324ms/step - loss: 6.2600 - val_loss: 6.2862\n",
            "Epoch 4/100\n",
            "154/154 [==============================] - 52s 339ms/step - loss: 6.2048 - val_loss: 6.2162\n",
            "Epoch 5/100\n",
            "154/154 [==============================] - 58s 376ms/step - loss: 6.1168 - val_loss: 6.1352\n",
            "Epoch 6/100\n",
            "154/154 [==============================] - 58s 376ms/step - loss: 6.0261 - val_loss: 6.0647\n",
            "Epoch 7/100\n",
            "154/154 [==============================] - 58s 379ms/step - loss: 5.9388 - val_loss: 5.9849\n",
            "Epoch 8/100\n",
            "154/154 [==============================] - 49s 318ms/step - loss: 5.8658 - val_loss: 5.9328\n",
            "Epoch 9/100\n",
            "154/154 [==============================] - 58s 377ms/step - loss: 5.8023 - val_loss: 5.8907\n",
            "Epoch 10/100\n",
            "154/154 [==============================] - 49s 318ms/step - loss: 5.7442 - val_loss: 5.8485\n",
            "Epoch 11/100\n",
            "154/154 [==============================] - 49s 322ms/step - loss: 5.6919 - val_loss: 5.8103\n",
            "Epoch 12/100\n",
            "154/154 [==============================] - 49s 319ms/step - loss: 5.6460 - val_loss: 5.7792\n",
            "Epoch 13/100\n",
            "154/154 [==============================] - 49s 318ms/step - loss: 5.5987 - val_loss: 5.7634\n",
            "Epoch 14/100\n",
            "154/154 [==============================] - 60s 394ms/step - loss: 5.5530 - val_loss: 5.7133\n",
            "Epoch 15/100\n",
            "154/154 [==============================] - 59s 382ms/step - loss: 5.5056 - val_loss: 5.7020\n",
            "Epoch 16/100\n",
            "154/154 [==============================] - 49s 317ms/step - loss: 5.4578 - val_loss: 5.6415\n",
            "Epoch 17/100\n",
            "154/154 [==============================] - 50s 322ms/step - loss: 5.4104 - val_loss: 5.6071\n",
            "Epoch 18/100\n",
            "154/154 [==============================] - 50s 322ms/step - loss: 5.3618 - val_loss: 5.5784\n",
            "Epoch 19/100\n",
            "154/154 [==============================] - 59s 383ms/step - loss: 5.3156 - val_loss: 5.5533\n",
            "Epoch 20/100\n",
            "154/154 [==============================] - 50s 323ms/step - loss: 5.2712 - val_loss: 5.5306\n",
            "Epoch 21/100\n",
            "154/154 [==============================] - 49s 320ms/step - loss: 5.2264 - val_loss: 5.5049\n",
            "Epoch 22/100\n",
            "154/154 [==============================] - 50s 323ms/step - loss: 5.1827 - val_loss: 5.4880\n",
            "Epoch 23/100\n",
            "154/154 [==============================] - 49s 321ms/step - loss: 5.1421 - val_loss: 5.4648\n",
            "Epoch 24/100\n",
            "154/154 [==============================] - 50s 325ms/step - loss: 5.1015 - val_loss: 5.4443\n",
            "Epoch 25/100\n",
            "154/154 [==============================] - 58s 376ms/step - loss: 5.0636 - val_loss: 5.4284\n",
            "Epoch 26/100\n",
            "154/154 [==============================] - 50s 323ms/step - loss: 5.0252 - val_loss: 5.4182\n",
            "Epoch 27/100\n",
            "154/154 [==============================] - 58s 379ms/step - loss: 4.9866 - val_loss: 5.3965\n",
            "Epoch 28/100\n",
            "154/154 [==============================] - 59s 383ms/step - loss: 4.9503 - val_loss: 5.3873\n",
            "Epoch 29/100\n",
            "154/154 [==============================] - 60s 387ms/step - loss: 4.9126 - val_loss: 5.3683\n",
            "Epoch 30/100\n",
            "154/154 [==============================] - 49s 322ms/step - loss: 4.8752 - val_loss: 5.3522\n",
            "Epoch 31/100\n",
            "154/154 [==============================] - 59s 383ms/step - loss: 4.8372 - val_loss: 5.3350\n",
            "Epoch 32/100\n",
            "154/154 [==============================] - 59s 381ms/step - loss: 4.8006 - val_loss: 5.3363\n",
            "Epoch 33/100\n",
            "154/154 [==============================] - 49s 319ms/step - loss: 4.7632 - val_loss: 5.3093\n",
            "Epoch 34/100\n",
            "154/154 [==============================] - 59s 382ms/step - loss: 4.7276 - val_loss: 5.2965\n",
            "Epoch 35/100\n",
            "154/154 [==============================] - 50s 327ms/step - loss: 4.6923 - val_loss: 5.2937\n",
            "Epoch 36/100\n",
            "154/154 [==============================] - 50s 324ms/step - loss: 4.6551 - val_loss: 5.2848\n",
            "Epoch 37/100\n",
            "154/154 [==============================] - 49s 320ms/step - loss: 4.6191 - val_loss: 5.2845\n",
            "Epoch 38/100\n",
            "154/154 [==============================] - 58s 376ms/step - loss: 4.5854 - val_loss: 5.2605\n",
            "Epoch 39/100\n",
            "154/154 [==============================] - 50s 325ms/step - loss: 4.5513 - val_loss: 5.2559\n",
            "Epoch 40/100\n",
            "154/154 [==============================] - 59s 381ms/step - loss: 4.5156 - val_loss: 5.2501\n",
            "Epoch 41/100\n",
            "154/154 [==============================] - 59s 383ms/step - loss: 4.4829 - val_loss: 5.2463\n",
            "Epoch 42/100\n",
            "154/154 [==============================] - 49s 322ms/step - loss: 4.4484 - val_loss: 5.2414\n",
            "Epoch 43/100\n",
            "154/154 [==============================] - 49s 320ms/step - loss: 4.4149 - val_loss: 5.2432\n",
            "Epoch 44/100\n",
            "154/154 [==============================] - 58s 378ms/step - loss: 4.3825 - val_loss: 5.2313\n",
            "Epoch 45/100\n",
            "154/154 [==============================] - 58s 374ms/step - loss: 4.3510 - val_loss: 5.2307\n",
            "Epoch 46/100\n",
            "154/154 [==============================] - 58s 375ms/step - loss: 4.3170 - val_loss: 5.2219\n",
            "Epoch 47/100\n",
            "154/154 [==============================] - 59s 383ms/step - loss: 4.2853 - val_loss: 5.2248\n",
            "Epoch 48/100\n",
            "154/154 [==============================] - 58s 379ms/step - loss: 4.2523 - val_loss: 5.2190\n",
            "Epoch 49/100\n",
            "154/154 [==============================] - 59s 381ms/step - loss: 4.2208 - val_loss: 5.2100\n",
            "Epoch 50/100\n",
            "154/154 [==============================] - 50s 326ms/step - loss: 4.1878 - val_loss: 5.2191\n",
            "Epoch 51/100\n",
            "154/154 [==============================] - 49s 320ms/step - loss: 4.1565 - val_loss: 5.2047\n",
            "Epoch 52/100\n",
            "154/154 [==============================] - 58s 379ms/step - loss: 4.1245 - val_loss: 5.2080\n",
            "Epoch 53/100\n",
            "154/154 [==============================] - 50s 323ms/step - loss: 4.0928 - val_loss: 5.2164\n",
            "Epoch 54/100\n",
            "154/154 [==============================] - 51s 330ms/step - loss: 4.0618 - val_loss: 5.2142\n",
            "Epoch 55/100\n",
            "154/154 [==============================] - 59s 384ms/step - loss: 4.0302 - val_loss: 5.2073\n",
            "Epoch 56/100\n",
            "154/154 [==============================] - 49s 319ms/step - loss: 3.9991 - val_loss: 5.2120\n",
            "Epoch 57/100\n",
            "154/154 [==============================] - 50s 323ms/step - loss: 3.9677 - val_loss: 5.2136\n",
            "Epoch 58/100\n",
            "154/154 [==============================] - 50s 324ms/step - loss: 3.9371 - val_loss: 5.2100\n",
            "Epoch 59/100\n",
            "154/154 [==============================] - 59s 386ms/step - loss: 3.9041 - val_loss: 5.2218\n",
            "Epoch 60/100\n",
            "154/154 [==============================] - 59s 382ms/step - loss: 3.8741 - val_loss: 5.2197\n",
            "Epoch 61/100\n",
            "154/154 [==============================] - 50s 328ms/step - loss: 3.8440 - val_loss: 5.2215\n",
            "Epoch 62/100\n",
            "154/154 [==============================] - 50s 323ms/step - loss: 3.8118 - val_loss: 5.2160\n",
            "Epoch 63/100\n",
            "154/154 [==============================] - 58s 380ms/step - loss: 3.7813 - val_loss: 5.2345\n",
            "Epoch 64/100\n",
            "154/154 [==============================] - 50s 328ms/step - loss: 3.7515 - val_loss: 5.2237\n",
            "Epoch 65/100\n",
            "154/154 [==============================] - 49s 320ms/step - loss: 3.7196 - val_loss: 5.2348\n",
            "Epoch 66/100\n",
            "154/154 [==============================] - 59s 381ms/step - loss: 3.6887 - val_loss: 5.2293\n",
            "Epoch 67/100\n",
            "154/154 [==============================] - 49s 320ms/step - loss: 3.6582 - val_loss: 5.2413\n",
            "Epoch 68/100\n",
            "154/154 [==============================] - 59s 383ms/step - loss: 3.6286 - val_loss: 5.2475\n",
            "Epoch 69/100\n",
            "154/154 [==============================] - 59s 385ms/step - loss: 3.5975 - val_loss: 5.2514\n",
            "Epoch 70/100\n",
            "154/154 [==============================] - 50s 324ms/step - loss: 3.5669 - val_loss: 5.2509\n",
            "Epoch 71/100\n",
            "154/154 [==============================] - 49s 320ms/step - loss: 3.5373 - val_loss: 5.2646\n",
            "Epoch 72/100\n",
            "154/154 [==============================] - 59s 384ms/step - loss: 3.5059 - val_loss: 5.2793\n",
            "Epoch 73/100\n",
            "154/154 [==============================] - 50s 326ms/step - loss: 3.4764 - val_loss: 5.2761\n",
            "Epoch 74/100\n",
            "154/154 [==============================] - 59s 383ms/step - loss: 3.4468 - val_loss: 5.2759\n",
            "Epoch 75/100\n",
            "154/154 [==============================] - 50s 323ms/step - loss: 3.4150 - val_loss: 5.2868\n",
            "Epoch 76/100\n",
            "154/154 [==============================] - 49s 319ms/step - loss: 3.3850 - val_loss: 5.2974\n",
            "Epoch 77/100\n",
            "154/154 [==============================] - 59s 381ms/step - loss: 3.3546 - val_loss: 5.3003\n",
            "Epoch 78/100\n",
            "154/154 [==============================] - 50s 322ms/step - loss: 3.3248 - val_loss: 5.3117\n",
            "Epoch 79/100\n",
            "154/154 [==============================] - 51s 330ms/step - loss: 3.2949 - val_loss: 5.3235\n",
            "Epoch 80/100\n",
            "154/154 [==============================] - 50s 324ms/step - loss: 3.2647 - val_loss: 5.3322\n",
            "Epoch 81/100\n",
            "154/154 [==============================] - 49s 321ms/step - loss: 3.2359 - val_loss: 5.3492\n",
            "Epoch 82/100\n",
            "154/154 [==============================] - 49s 321ms/step - loss: 3.2046 - val_loss: 5.3579\n",
            "Epoch 83/100\n",
            "154/154 [==============================] - 59s 382ms/step - loss: 3.1747 - val_loss: 5.3572\n",
            "Epoch 84/100\n",
            "154/154 [==============================] - 50s 328ms/step - loss: 3.1449 - val_loss: 5.3670\n",
            "Epoch 85/100\n",
            "154/154 [==============================] - 50s 323ms/step - loss: 3.1145 - val_loss: 5.3738\n",
            "Epoch 86/100\n",
            "154/154 [==============================] - 49s 318ms/step - loss: 3.0834 - val_loss: 5.3897\n",
            "Epoch 87/100\n",
            "154/154 [==============================] - 58s 379ms/step - loss: 3.0533 - val_loss: 5.4040\n",
            "Epoch 88/100\n",
            "154/154 [==============================] - 49s 322ms/step - loss: 3.0242 - val_loss: 5.3998\n",
            "Epoch 89/100\n",
            "154/154 [==============================] - 50s 326ms/step - loss: 2.9945 - val_loss: 5.4208\n",
            "Epoch 90/100\n",
            "154/154 [==============================] - 60s 388ms/step - loss: 2.9656 - val_loss: 5.4301\n",
            "Epoch 91/100\n",
            "154/154 [==============================] - 58s 380ms/step - loss: 2.9335 - val_loss: 5.4352\n",
            "Epoch 92/100\n",
            "154/154 [==============================] - 49s 320ms/step - loss: 2.9046 - val_loss: 5.4475\n",
            "Epoch 93/100\n",
            "154/154 [==============================] - 50s 328ms/step - loss: 2.8746 - val_loss: 5.4619\n",
            "Epoch 94/100\n",
            "154/154 [==============================] - 50s 325ms/step - loss: 2.8472 - val_loss: 5.4741\n",
            "Epoch 95/100\n",
            "154/154 [==============================] - 51s 329ms/step - loss: 2.8145 - val_loss: 5.4731\n",
            "Epoch 96/100\n",
            "154/154 [==============================] - 59s 381ms/step - loss: 2.7860 - val_loss: 5.4917\n",
            "Epoch 97/100\n",
            "154/154 [==============================] - 50s 327ms/step - loss: 2.7556 - val_loss: 5.5065\n",
            "Epoch 98/100\n",
            "154/154 [==============================] - 59s 383ms/step - loss: 2.7242 - val_loss: 5.5149\n",
            "Epoch 99/100\n",
            "154/154 [==============================] - 58s 377ms/step - loss: 2.6949 - val_loss: 5.5311\n",
            "Epoch 100/100\n",
            "154/154 [==============================] - 50s 328ms/step - loss: 2.6648 - val_loss: 5.5532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the input sequence to get the \"thought vectors\"\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2) # A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2)\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index['START_']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += ' '+sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '_END' or\n",
        "           len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "train_gen = generate_batch(X_train, y_train, batch_size = 1)\n",
        "k=-1"
      ],
      "metadata": {
        "id": "DFN-y70WoxnE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', X_train[k:k+1].values[0])\n",
        "print('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\n",
        "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
      ],
      "metadata": {
        "id": "Lcn_CAoBoxqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b657a59-6ab9-4ff5-9e6c-d8ae602dcaf0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Input English sentence: the problem is these are arousal addictions\n",
            "Actual Hindi Translation:  मुख्य समस्या इन उत्तेजक व्यसनों से है \n",
            "Predicted Hindi Translation:  ये समस्या है। इन देश के बीच भी है। \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXix-Y__wOgV"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}